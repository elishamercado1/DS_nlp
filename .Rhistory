install.packages("SnowballC")
install.packages("textstem")
install.packages("tm")
install.packages("reshape2")
example_one <-  c("I like NLP", "Regular expression looks complicated", "Regular expression looks complicated", "NLP")
grep(pattern = "NLP", example_one)
grepl(pattern = "NLP", example_one)
example_two <- c("we are looking at different regex", "i am enjoying it", "it looks looking fun now")
regexpr(pattern = "look", example_two)
gregexpr(pattern = "look", example_two)
exercise <- c("abc@ons.gov.uk", "ttt@mod.gov.uk", "abc@hotmail.com", "nna@gmail.com", "adead@ons ons.uk")
grepl("ons", ignore.case = TRUE)
grepl("ons", exercise, ignore.case = TRUE)
gregexpr("ons", exercise, ignore.case = TRUE)
string_position <- gregexpr("ons", exercise, ignore.case = TRUE)[1]
View(string_position)
string_position <- gregexpr("ons", exercise, ignore.case = TRUE)
View(string_position)
test <- c ("tin", "tan", "tun", "ton", "ten")
str_view( test, "t[aeiou]n")
library(stringr)
test <- c ("tin", "tan", "tun", "ton", "ten")
str_view( test, "t[aeiou]n")
test <- c ("tin", "tan", "turn", "tain","t.n", "tennis" )
str_view(test, "t[aeiou]n")
# 7. EXERCISE -------------------------------------------------------------
character_set_example <- c ("is there any solution?", ".[{(^$|?*+", "coreyms.com",
"321-555-4321", "123.555.1234", "This is an awesome album")
str_view(character_set_example, "[0-9]")
str_view(character_set_example, "([^a-z]i)|(^[0-9])")
str_view(character_set_example, "([^a-z]\i)|(^[0-9])")
str_view(character_set_example, "([^a-z]?i)|(^[0-9])")
str_view(character_set_example, "([^a-z])|(^[0-9])|([^A-Z])")
str_view(character_set_example, "[0-9]")
str_view(character_set_example, "([^a-z])&(^[0-9])&([^A-Z])")
str_view(character_set_example, "([^a-z]&^[0-9]&[^A-Z])")
str_view(character_set_example, "[^0-9a-zA-Z]")
#^ at the start of a character set excludes the next characters,
# at the end of a character set it just represents itself
caret_end <- c(1,5,7,"hello", "22", "testing","^")
str_view(caret_end, "[0-9^]")
# 8. CHARACTER CLASSES ----------------------------------------------------
character_class_example <- c("We are learning How regex works", "lets see", "testing", "hello world", "2011", "7687687", "Bank holiday 2021")
str_view(character_class_example, "\\s")
str_view(example_three_character, "[0-9]{2}")
# 8.1 EXERCISE
example_three_character <- c(1,2,544,"654")
str_view(example_three_character, "[0-9]{2}")
# 9 POSIX CHARACTER CLASS -------------------------------------------------
# POSIX character classes are predefined sequences for a certain set of characters.
# Double brackets are needed
posix_example <- c(1,55,7,"hello", "22", "apple", "banana", "pear","^")
str_replace_all(posix_example, "[[:digit:]]", "testing")
str_replace(posix_example, "[[:digit:]]", "testing")
# 9.1 EXERCISE
posix_exercise <- c("Hello, world!", "This is bliss", "321-555-4321", "it is    raining")
str_replace_all(posix_exercise, "[[:space:]]", "")
# 9. ANCHORS --------------------------------------------------------------
top_university <- c("Harvard University", "University of Cambridge", "Columbia University", "University of Oxford", "Yale University", "Stanford University")
str_view_all(top_university, ("^University"))
str_view_all(top_university, ("University$"))
# 11. EXERCISE ------------------------------------------------------------
email_exercise <- c("abc@ons.gov.uk", "ttt@mod.gov.uk", "abc@hotmail.com", "nna@gmail.com", "adead@ons.uk")
str_view_all(email_exercise, ("\@ons\.uk$"))
str_view_all(email_exercise, ("@ons\.uk$"))
str_view_all(email_exercise, ("@ons\\.uk$"))
str_view_all(email_exercise, ("@ons.gov.uk$"))
# 12.1 EXERCISE
quantifier_exercise <- c(" I bought 2 tickets 2-613-213-4567 or 5555555555",
"43 Apple Rd, Orange QC POA 5RQ - 613 321 4567",
"contact Dr who (613)2134567",
"1.575.555.5555 is  #1 number",
"7164347566",
" 1234567 cats"
)
str_view_all(quantifier_exercise, "([0-9]|.|\\(|\\)|-|\\s){5+}")
str_view_all(quantifier_exercise, "([0-9]|.|\\(|\\)|-|\\s){5}+")
library(tidyverse)
library(tidytext)
library(SnowballC)
library(textstem)
library(tidyverse)
library(tidytext)
library(SnowballC)  # for stemming
library(textstem)   # for lemmatization
patents <- readr::read_csv("../data/patent.csv")
patents <- readr::read_csv("./data/patent.csv")
knitr::kable(head(patents), align = "lccrr")
tidy_patent<-patents %>%
unnest_tokens(input= abstract,
output= "word",
token= "words")
tidy_patent
# Can also do other tokenisation characters, n_gram, sentences, lines, paragraphs, regex
tidy_patent %>%
count(word, sort= TRUE)
rumi_poem <- c("Do you know what you are?  You are a manuscript of a divine letter .",
"22 334 565 343 You are a mirror reflecting a noble face.",
"This universe is not outside of you. Look inside yourself." ,
"everything that you want 55 43 35,",
"you are already that.")
# Creating a tibble
rumi_poem_df <- tibble(line = 1:length(rumi_poem), text = rumi_poem)
kable(rumi_poem_df)
glimpse(rumi_poem_df)
rumi_sentences <- rumi_poem_df |>
unnest_tokens(text, "sentence", "sentences")
rumi_sentences <- rumi_poem_df |>
unnest_tokens(text, "sentence", "sentence")
rumi_sentences <- rumi_poem_df |>
unnest_tokens(input = text, output = "sentence", token = "sentences")
View(rumi_sentences)
# 3.3 STOP WORDS ----------------------------------------------------------
stop_words
tidy_patent <-tidy_patent %>%
anti_join(stop_words, by = "word") # removing stop words
tidy_patent
top_words <- tidy_patent %>%
count(word, sort = TRUE) %>% #the output has 2 columns word and n (the counts of each word)
top_n(10) %>% #select the 10 top rows
mutate(word = reorder(word, n))
top_words
ggplot(top_words, aes(x = word, y = n), group= 1) +
geom_col(fill = "blue") +
theme(legend.position = "none",
plot.title = element_text(hjust = 0.5),
panel.grid.major = element_blank()) +
xlab("") +
ylab("Word Count") +
ggtitle("Most frequently used words ") +
coord_flip()
# 3.3.1 CUSTOM STOP WORDS
custom_stop_words <- bind_rows(tibble(word = c("includes", "oohhh", "lolllll"), lexicon=c("custom")), stop_words)
View(custom_stop_words)
tidy_patent <-tidy_patent %>%
anti_join(custom_stop_words, by = "word") # removing stop words
tidy_patent
rumi_poem_df
rumi_words <- rumi_poem_df |>
unnest_tokens(input = text, output = "word", token = "words") |>
anti_join(stop_words, by = "word")
View(rumi_words)
# 3.5 REMOVING PUNCTUATION ------------------------------------------------
sample_text <- "Let's see How punctuation : , FuNcTiON woRkS ! "
str_replace_all(sample_text, '[[:punct:]]+', ' ')
# SOMETIMES WE WANT TO RETAIN PUNCTUATION! USE STRIP_PUNCT = FALSE PARAMETER FROM UNNEST_TOKENS
one_abstract <- c("!! { } - : A buffing-pad that includes a substantially conventional buffing pad made of tufted wool!",
"that includes: backing plate,  a central hub for attachment to the shaft of a {rotary power buffer} :",
"The pad also carries a plurality of shorter tufts of wool or other fiber of a contrasting color.")
abstract_example <- tibble(line = 1:length(one_abstract), text = one_abstract)
abstract_example
with_punct <- abstract_example %>%
unnest_tokens(input= text,
output= "word",
token= "words",
strip_punct=FALSE)
with_punct
without_punct <- abstract_example %>%
unnest_tokens(input= text,
output= "word",
token= "words")
without_punct
# 3.6 CONVERT TEXT TO LOWERCASE -------------------------------------------
sample_text <- "Let's see How lowercaSe FuNcTiON woRkS ! "
tolower(sample_text)
# 3.7 REMOVING NUMBERS ----------------------------------------------------
number <- tidy_patent %>%
filter (str_detect(word, "^[0-9]"))
tidy_patent <- tidy_patent%>%
anti_join(number, by="word")
tidy_patent
View(rumi_words)
# 3.8 EXERCISE ------------------------------------------------------------
rumi_no_numbers <- rumi_words |>
filter(str_detect(word, "^[0-9]"))
View(rumi_no_numbers)
# 3.7 REMOVING NUMBERS ----------------------------------------------------
number <- tidy_patent %>%
filter (str_detect(word, "^[0-9]"))
# 3.8 EXERCISE ------------------------------------------------------------
rumi_no_numbers <- rumi_words |>
filter(str_detect(word, "[0-9]"))
View(rumi_no_numbers)
# 3.8 EXERCISE ------------------------------------------------------------
rumi_numbers <- rumi_words |>
filter(str_detect(word, "^[0-9]"))
rumi_no_numbers <- rumi_words |>
anti_join(rumi_no_numbers, by = "word")
View(rumi_no_numbers)
# REMOVING WHITESPACES ----------------------------------------------------
tidy_patent <- tidy_patent %>%
str_remove_all(word, "[[:blank:]]")
# REMOVING WHITESPACES ----------------------------------------------------
tidy_patent <- tidy_patent %>%
str_remove_all(word, "[[:blank:]]")
tidy_patent
# REMOVING WHITESPACES ----------------------------------------------------
tidy_patent <- tidy_patent %>%
str_remove_all(word, "[[:blank:]]")
# REMOVING WHITESPACES ----------------------------------------------------
tidy_patent %>%
str_remove_all(word, "[[:blank:]]")
# REMOVING WHITESPACES ----------------------------------------------------
tidy_patent <- tidy_patent %>%
str_remove_all(word, "[[:blank:]]")
# REMOVING WHITESPACES ----------------------------------------------------
tidy_patent <- tidy_patent %>%
str_remove(word, "[[:blank:]]")
# REMOVING WHITESPACES ----------------------------------------------------
tidy_patent <-tidy_patent %>%
anti_join(custom_stop_words, by = "word") # removing stop words
tidy_patent
tidy_patent <- tidy_patent %>%
str_remove_all(word, "[[:blank:]]")
tidy_patent<-patents %>%
unnest_tokens(input= abstract,
output= "word",
token= "words")
tidy_patent <- tidy_patent %>%
str_remove_all(word, "[[:blank:]]")
